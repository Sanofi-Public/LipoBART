{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ebf85-1d13-468c-b504-58ec150593e2",
   "metadata": {},
   "source": [
    "# Train and Test a CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fad9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57682300-d773-4cd3-9b55-d64645a7ef53",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c765b1c-22de-4794-939d-7041c6f5521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.to_dict(orient=\"records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "lnps = load_data(\"../data/ding_et_al/all_data.csv\")\n",
    "with open(\"../data/ding_et_al/split.json\") as f:\n",
    "    split_df = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This function we use for 'our' methods ---\n",
    "# we simply generate a feature matrix using only the embeddings from the m1 molecule\n",
    "def generate_simple_feature_matrix(data_df, fp_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    processed_data = []\n",
    "    for item in data_df:\n",
    "        result = {}\n",
    "        result[\"label\"] = item[\"y2\"]\n",
    "        result[\"m1_fingerprint\"] = fp_dict[item[\"m1\"]]\n",
    "        processed_data.append(result)\n",
    "        X_item = result[\"m1_fingerprint\"]\n",
    "        X.append(X_item)\n",
    "        y.append(item[\"y2\"])\n",
    "    return (X, y, processed_data)\n",
    "\n",
    "\n",
    "# -- Functions from Ding et al. to process data --\n",
    "def convert_to_one_hot(val, min, max, step):\n",
    "    result = []\n",
    "    for i in range(int((max - min) / step)):\n",
    "        if i * step <= val < (i + 1) * step:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_feature_matrix(data_df, fp_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    processed_data = []\n",
    "\n",
    "    for item in data_df:\n",
    "        result = {}\n",
    "        result[\"label\"] = item[\"y2\"]\n",
    "        result[\"p1_feature\"] = convert_to_one_hot(item[\"p1\"], min=0, max=100, step=5)\n",
    "        result[\"p2_feature\"] = convert_to_one_hot(item[\"p2\"], min=0, max=100, step=5)\n",
    "        result[\"p3_feature\"] = convert_to_one_hot(item[\"p3\"], min=0, max=100, step=5)\n",
    "        result[\"p4_feature\"] = convert_to_one_hot(item[\"p4\"], min=0, max=1.5, step=0.25)\n",
    "        result[\"m1_fingerprint\"] = fp_dict[item[\"m1\"]]\n",
    "        result[\"m2_fingerprint\"] = fp_dict[item[\"m2\"]]\n",
    "        result[\"m3_fingerprint\"] = fp_dict[item[\"m3\"]]\n",
    "        result[\"m4_fingerprint\"] = fp_dict[item[\"m4\"]]\n",
    "\n",
    "        processed_data.append(result)\n",
    "        X_item = (\n",
    "            result[\"p1_feature\"]\n",
    "            + result[\"p2_feature\"]\n",
    "            + result[\"p3_feature\"]\n",
    "            + result[\"p4_feature\"]\n",
    "            + result[\"m1_fingerprint\"]\n",
    "            + result[\"m2_fingerprint\"]\n",
    "            + result[\"m3_fingerprint\"]\n",
    "            + result[\"m4_fingerprint\"]\n",
    "        )\n",
    "        X.append(X_item)\n",
    "        y.append(item[\"y2\"])\n",
    "    return (X, y, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load in fingerprints --- #\n",
    "with open(\"../data/ding_et_al/mol2fp_grover_large.json\", \"r\") as f:\n",
    "    df_fp_grover_large = json.load(f)\n",
    "with open(\"../data/ding_et_al/mol2fp_grover.json\", \"r\") as f:\n",
    "    df_fp_grover = json.load(f)\n",
    "with open(\"../data/ding_et_al/mol2fp.json\", \"r\") as f:\n",
    "    df_fp = json.load(f)\n",
    "with open(\"../data/mol2fp_cfp_all_data.json\", \"r\") as f:\n",
    "    df_fp_cfp = json.load(f)\n",
    "with open(\"../data/mol2fp_MegaMB_base_all_data.json\", \"r\") as f:\n",
    "    df_fp_mmb = json.load(f)\n",
    "with open(\"../data/mol2fp_MegaMB_finetuned_all_data.json\", \"r\") as f:\n",
    "    df_fp_mmb_ft = json.load(f)\n",
    "gcn_X = np.load(\"../data/gcn_x.npy\")\n",
    "\n",
    "fp_X, y, _ = generate_feature_matrix(lnps, df_fp)\n",
    "grover_X, _, _ = generate_feature_matrix(lnps, df_fp_grover)\n",
    "grover_large_X, _, _ = generate_feature_matrix(lnps, df_fp_grover_large)\n",
    "cfp_X, _, _ = generate_simple_feature_matrix(lnps, df_fp_cfp)\n",
    "mmb_X, _, _ = generate_simple_feature_matrix(lnps, df_fp_mmb)\n",
    "mmb_ft_X, _, _ = generate_simple_feature_matrix(lnps, df_fp_mmb_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make Hybrid fingerprints ----\n",
    "fp_grover_X = pd.concat([pd.DataFrame(fp_X), pd.DataFrame(grover_X)], axis=1)\n",
    "fp_grover_large_X = pd.concat([pd.DataFrame(fp_X), pd.DataFrame(grover_large_X)], axis=1)\n",
    "cfp_mmb_ft_X = pd.concat([pd.DataFrame(cfp_X), pd.DataFrame(mmb_ft_X)], axis=1)\n",
    "gcn_cfp_X = pd.concat([pd.DataFrame(gcn_X), pd.DataFrame(cfp_X)], axis=1)\n",
    "gcn_mmb_ft_X = pd.concat([pd.DataFrame(gcn_X), pd.DataFrame(mmb_ft_X)], axis=1)\n",
    "gcn_mmb_ft_cfp = pd.concat(\n",
    "    [pd.DataFrame(gcn_X), pd.DataFrame(mmb_ft_X), pd.DataFrame(cfp_X)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71de758",
   "metadata": {},
   "source": [
    "## Train and test CatBoost\n",
    "> Use different embeddings for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62834352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_catboost(parameter_grid, X, y, split_df):\n",
    "    X_train = np.array(X)[split_df[\"train\"],]\n",
    "    X_val = np.array(X)[split_df[\"val\"],]\n",
    "    X_test = np.array(X)[split_df[\"test\"],]\n",
    "    y_train = np.array(y)[split_df[\"train\"]]\n",
    "    y_val = np.array(y)[split_df[\"val\"]]\n",
    "    y_test = np.array(y)[split_df[\"test\"]]\n",
    "\n",
    "    val_auc = []\n",
    "    test_auc = []\n",
    "    val_acc = []\n",
    "    test_acc = []\n",
    "    val_f1 = []\n",
    "    test_f1 = []\n",
    "    val_mcc = []\n",
    "    test_mcc = []\n",
    "    for depth in parameter_grid[\"depth\"]:\n",
    "        for learning_rate in parameter_grid[\"learning_rate\"]:\n",
    "            for iterations in parameter_grid[\"iterations\"]:\n",
    "                model = CatBoostClassifier(\n",
    "                    depth=depth,\n",
    "                    iterations=iterations,\n",
    "                    learning_rate=learning_rate,\n",
    "                    verbose=False,\n",
    "                    random_seed=random_seed,\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                model_prob_val = model.predict_proba(X_val)\n",
    "                model_prob_test = model.predict_proba(X_test)\n",
    "                val_auc.append(roc_auc_score(y_val, model_prob_val[:, 1]))\n",
    "                test_auc.append(roc_auc_score(y_test, model_prob_test[:, 1]))\n",
    "                pred_val = model.predict(X_val)\n",
    "                pred_test = model.predict(X_test)\n",
    "                val_acc.append(balanced_accuracy_score(y_val, pred_val))\n",
    "                test_acc.append(balanced_accuracy_score(y_test, pred_test))\n",
    "                val_f1.append(f1_score(y_val, pred_val))\n",
    "                test_f1.append(f1_score(y_test, pred_test))\n",
    "                val_mcc.append(matthews_corrcoef(y_val, pred_val))\n",
    "                test_mcc.append(matthews_corrcoef(y_test, pred_test))\n",
    "    return (val_auc, test_auc, val_acc, test_acc, val_f1, test_f1, val_mcc, test_mcc)\n",
    "\n",
    "\n",
    "param_grid = {\"depth\": [3, 4, 5], \"learning_rate\": [0.01, 0.1], \"iterations\": [1000, 2000, 3000]}\n",
    "\n",
    "\n",
    "def catboost(X, y, split_df):\n",
    "    \"\"\"\n",
    "    Run Catboost without any hparam optimization\n",
    "    \"\"\"\n",
    "    X_train = np.array(X)[split_df[\"train\"],]\n",
    "    X_val = np.array(X)[split_df[\"valid\"],]\n",
    "    X_test = np.array(X)[split_df[\"test\"],]\n",
    "    y_train = np.array(y)[split_df[\"train\"]]\n",
    "    y_val = np.array(y)[split_df[\"valid\"]]\n",
    "    y_test = np.array(y)[split_df[\"test\"]]\n",
    "    # concatenate train and val\n",
    "    X_train = np.concatenate((X_train, X_val), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "    model = CatBoostClassifier(\n",
    "        depth=5, iterations=5000, learning_rate=0.01, verbose=False, random_seed=random_seed\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    model_prob_val = model.predict_proba(X_val)\n",
    "    model_prob_test = model.predict_proba(X_test)\n",
    "    val_auc = roc_auc_score(y_val, model_prob_val[:, 1])\n",
    "    test_auc = roc_auc_score(y_test, model_prob_test[:, 1])\n",
    "    pred_val = model.predict(X_val)\n",
    "    pred_test = model.predict(X_test)\n",
    "    val_acc = balanced_accuracy_score(y_val, pred_val)\n",
    "    test_acc = balanced_accuracy_score(y_test, pred_test)\n",
    "    val_f1 = f1_score(y_val, pred_val)\n",
    "    test_f1 = f1_score(y_test, pred_test)\n",
    "    return (val_auc, test_auc, val_acc, test_acc, val_f1, test_f1)\n",
    "\n",
    "\n",
    "def run_catboost(X, y, split_df, parameter_grid):\n",
    "    val_auc, test_auc, val_acc, test_acc, val_f1, test_f1, val_mcc, test_mcc = select_catboost(\n",
    "        parameter_grid, X, y, split_df\n",
    "    )\n",
    "    # --- Compute the best result from the validation set ---\n",
    "    best_idx = np.argmax(val_auc)\n",
    "    val_auc, test_auc, val_acc, test_acc, val_f1, test_f1, val_mcc, test_mcc = (\n",
    "        val_auc[best_idx],\n",
    "        test_auc[best_idx],\n",
    "        val_acc[best_idx],\n",
    "        test_acc[best_idx],\n",
    "        val_f1[best_idx],\n",
    "        test_f1[best_idx],\n",
    "        val_mcc[best_idx],\n",
    "        test_mcc[best_idx],\n",
    "    )\n",
    "    print(\"VAL\")\n",
    "    print(f\"AUC: {val_auc}\")\n",
    "    print(f\"Balanced Accuracy: {val_acc}\")\n",
    "    print(f\"F1: {val_f1}\")\n",
    "    print(f\"MCC: {val_mcc}\")\n",
    "    print(\"TEST\")\n",
    "    print(f\"AUC: {test_auc}\")\n",
    "    print(f\"Balanced Accuracy: {test_acc}\")\n",
    "    print(f\"F1: {test_f1}\")\n",
    "    print(f\"MCC: {test_mcc}\")\n",
    "    # latex table row\n",
    "    print(\n",
    "        f\"{val_auc:.3f} & {test_auc:.3f} & {val_acc:.3f} & {test_acc:.3f} & {val_f1:.3f} & {test_f1:.3f} & {val_mcc:.3f} & {test_mcc:.3f} \\\\\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c809774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expert --- #\n",
    "run_catboost(fp_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MegaMolBART  base --- #\n",
    "run_catboost(mmb_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MegaMolBART Fine-tuned #\n",
    "run_catboost(mmb_ft_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0763016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CFP --- #\n",
    "run_catboost(cfp_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301adf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grover --- #\n",
    "run_catboost(grover_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3274bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CFP-MMB Fine-tuned --- #\n",
    "run_catboost(cfp_mmb_ft_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expert-Grover --- #\n",
    "run_catboost(fp_grover_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9508d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grover-Large --- #\n",
    "run_catboost(grover_large_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expert-Grover-Large --- #\n",
    "run_catboost(fp_grover_large_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN --- #\n",
    "run_catboost(gcn_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN-CFP --- #\n",
    "run_catboost(gcn_cfp_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN-MMB Fine-tuned --- #\n",
    "run_catboost(gcn_mmb_ft_X, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a614b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN-MMB Fine-tuned-CFP --- #\n",
    "run_catboost(gcn_mmb_ft_cfp, y, split_df, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f738514-2df6-4617-ad25-f8e2f3b2a686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lnp",
   "language": "python",
   "name": "lnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cb424124a72f34ff51dcdc06273546de873365fee37dc5a1ddc2de1ace99d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
