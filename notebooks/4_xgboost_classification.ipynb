{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a0ec87-4e9e-45a6-a4ae-91fa3b1c6a69",
   "metadata": {},
   "source": [
    "# Train and Test a XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ce31a-65cf-4449-ae5d-7e12f19c4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fa3cf-23f4-4958-aa29-19e9812bc772",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad2d79-28b9-4854-b174-a9a43f2bf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.to_dict(orient=\"records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83545a86-9321-4fe7-8485-d185650280d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "lnps = load_data(\"../data/ding_et_al/all_data.csv\")\n",
    "with open(\"../data/ding_et_al/split.json\") as f:\n",
    "    split_df = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55f88e-32a0-472f-a489-869d366f0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This function we use for 'our' methods ---\n",
    "# we simply generate a feature matrix using only the embeddings from the m1 molecule\n",
    "def generate_simple_feature_matrix(data_df, fp_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    processed_data = []\n",
    "    for item in data_df:\n",
    "        result = {}\n",
    "        result[\"label\"] = item[\"y2\"]\n",
    "        result[\"m1_fingerprint\"] = fp_dict[item[\"m1\"]]\n",
    "        processed_data.append(result)\n",
    "        X_item = result[\"m1_fingerprint\"]\n",
    "        X.append(X_item)\n",
    "        y.append(item[\"y2\"])\n",
    "    return (X, y, processed_data)\n",
    "\n",
    "\n",
    "# -- Functions from Ding et al. to process data --\n",
    "def convert_to_one_hot(val, min, max, step):\n",
    "    result = []\n",
    "    for i in range(int((max - min) / step)):\n",
    "        if i * step <= val < (i + 1) * step:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_feature_matrix(data_df, fp_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    processed_data = []\n",
    "\n",
    "    for item in data_df:\n",
    "        result = {}\n",
    "        result[\"label\"] = item[\"y2\"]\n",
    "        result[\"p1_feature\"] = convert_to_one_hot(item[\"p1\"], min=0, max=100, step=5)\n",
    "        result[\"p2_feature\"] = convert_to_one_hot(item[\"p2\"], min=0, max=100, step=5)\n",
    "        result[\"p3_feature\"] = convert_to_one_hot(item[\"p3\"], min=0, max=100, step=5)\n",
    "        result[\"p4_feature\"] = convert_to_one_hot(item[\"p4\"], min=0, max=1.5, step=0.25)\n",
    "        result[\"m1_fingerprint\"] = fp_dict[item[\"m1\"]]\n",
    "        result[\"m2_fingerprint\"] = fp_dict[item[\"m2\"]]\n",
    "        result[\"m3_fingerprint\"] = fp_dict[item[\"m3\"]]\n",
    "        result[\"m4_fingerprint\"] = fp_dict[item[\"m4\"]]\n",
    "\n",
    "        processed_data.append(result)\n",
    "        X_item = (\n",
    "            result[\"p1_feature\"]\n",
    "            + result[\"p2_feature\"]\n",
    "            + result[\"p3_feature\"]\n",
    "            + result[\"p4_feature\"]\n",
    "            + result[\"m1_fingerprint\"]\n",
    "            + result[\"m2_fingerprint\"]\n",
    "            + result[\"m3_fingerprint\"]\n",
    "            + result[\"m4_fingerprint\"]\n",
    "        )\n",
    "        X.append(X_item)\n",
    "        y.append(item[\"y2\"])\n",
    "    return (X, y, processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b93e2-0f12-41bf-8a23-1daaaea5410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load in fingerprints --- #\n",
    "with open(\"../data/ding_et_al/mol2fp_grover_large.json\", \"r\") as f:\n",
    "    df_fp_grover_large = json.load(f)\n",
    "with open(\"../data/ding_et_al/mol2fp_grover.json\", \"r\") as f:\n",
    "    df_fp_grover = json.load(f)\n",
    "with open(\"../data/ding_et_al/mol2fp.json\", \"r\") as f:\n",
    "    df_fp = json.load(f)\n",
    "with open(\"../data/mol2fp_cfp_all_data.json\", \"r\") as f:\n",
    "    df_fp_cfp = json.load(f)\n",
    "with open(\"../data/mol2fp_MegaMB_base_all_data.json\", \"r\") as f:\n",
    "    df_fp_mmb = json.load(f)\n",
    "with open(\"../data/mol2fp_MegaMB_finetuned_all_data.json\", \"r\") as f:\n",
    "    df_fp_mmb_ft = json.load(f)\n",
    "gcn_X = np.load(\"../data/gcn_x.npy\")\n",
    "\n",
    "fp_X, y, _ = generate_feature_matrix(lnps, df_fp)\n",
    "grover_X, _, _ = generate_feature_matrix(lnps, df_fp_grover)\n",
    "grover_large_X, _, _ = generate_feature_matrix(lnps, df_fp_grover_large)\n",
    "cfp_X, _, _ = generate_simple_feature_matrix(lnps, df_fp_cfp)\n",
    "mmb_X, _, _ = generate_simple_feature_matrix(lnps, df_fp_mmb)\n",
    "mmb_ft_X, _, _ = generate_simple_feature_matrix(lnps, df_fp_mmb_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a5da4b-a216-4555-ba88-0619ed39f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make Hybrid fingerprints ----\n",
    "fp_grover_X = pd.concat([pd.DataFrame(fp_X), pd.DataFrame(grover_X)], axis=1)\n",
    "fp_grover_large_X = pd.concat([pd.DataFrame(fp_X), pd.DataFrame(grover_large_X)], axis=1)\n",
    "cfp_mmb_ft_X = pd.concat([pd.DataFrame(cfp_X), pd.DataFrame(mmb_ft_X)], axis=1)\n",
    "gcn_cfp_X = pd.concat([pd.DataFrame(gcn_X), pd.DataFrame(cfp_X)], axis=1)\n",
    "gcn_mmb_ft_X = pd.concat([pd.DataFrame(gcn_X), pd.DataFrame(mmb_ft_X)], axis=1)\n",
    "gcn_mmb_ft_cfp = pd.concat(\n",
    "    [pd.DataFrame(gcn_X), pd.DataFrame(mmb_ft_X), pd.DataFrame(cfp_X)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab1b7d",
   "metadata": {},
   "source": [
    "## Train and test XGBoost\n",
    "> Use different embeddings for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_xgboost(parameter_grid, X, y, split_df):\n",
    "    X_train = np.array(X)[split_df[\"train\"],]\n",
    "    X_val = np.array(X)[split_df[\"val\"],]\n",
    "    X_test = np.array(X)[split_df[\"test\"],]\n",
    "    y_train = np.array(y)[split_df[\"train\"]]\n",
    "    y_val = np.array(y)[split_df[\"val\"]]\n",
    "    y_test = np.array(y)[split_df[\"test\"]]\n",
    "\n",
    "    val_auc = []\n",
    "    test_auc = []\n",
    "    val_acc = []\n",
    "    test_acc = []\n",
    "    val_f1 = []\n",
    "    test_f1 = []\n",
    "    val_mcc = []\n",
    "    test_mcc = []\n",
    "    for n_estimators in xgb_grid[\"n_estimators\"]:\n",
    "        clf = xgb.XGBClassifier(\n",
    "            random_state=3, n_estimators=n_estimators, verbosity=0, silent=True\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        clf_pred_val = clf.predict_proba(X_val)\n",
    "        clf_pred_test = clf.predict_proba(X_test)\n",
    "        val_auc.append(roc_auc_score(y_val, clf_pred_val[:, 1]))\n",
    "        test_auc.append(roc_auc_score(y_test, clf_pred_test[:, 1]))\n",
    "        pred_val = clf.predict(X_val)\n",
    "        pred_test = clf.predict(X_test)\n",
    "        val_acc.append(balanced_accuracy_score(y_val, pred_val))\n",
    "        test_acc.append(balanced_accuracy_score(y_test, pred_test))\n",
    "        val_f1.append(f1_score(y_val, pred_val))\n",
    "        test_f1.append(f1_score(y_test, pred_test))\n",
    "        val_mcc.append(matthews_corrcoef(y_val, pred_val))\n",
    "        test_mcc.append(matthews_corrcoef(y_test, pred_test))\n",
    "    return (val_auc, test_auc, val_acc, test_acc, val_f1, test_f1, val_mcc, test_mcc)\n",
    "\n",
    "\n",
    "def run_xgb(X, y, split_df, parameter_grid):\n",
    "    val_auc, test_auc, val_acc, test_acc, val_f1, test_f1, val_mcc, test_mcc = select_xgboost(\n",
    "        parameter_grid, X, y, split_df\n",
    "    )\n",
    "    best_idx = np.argmax(val_auc)\n",
    "    print(\"VAL\")\n",
    "    print(f\"AUC: {val_auc[best_idx]}\")\n",
    "    print(f\"Balanced Accuracy: {val_acc[best_idx]}\")\n",
    "    print(f\"F1: {val_f1[best_idx]}\")\n",
    "    print(f\"MCC: {val_mcc[best_idx]}\")\n",
    "    print(\"TEST\")\n",
    "    print(f\"AUC: {test_auc[best_idx]}\")\n",
    "    print(f\"Balanced Accuracy: {test_acc[best_idx]}\")\n",
    "    print(f\"F1: {test_f1[best_idx]}\")\n",
    "    print(f\"MCC: {test_mcc[best_idx]}\")\n",
    "\n",
    "    # Latex table row\n",
    "    print(\n",
    "        f\"{val_auc[best_idx]:.3f} & {test_auc[best_idx]:.3f} & {val_acc[best_idx]:.3f} & {test_acc[best_idx]:.3f} & {val_f1[best_idx]:.3f} & {test_f1[best_idx]:.3f} & {val_mcc[best_idx]:.3f} & {test_mcc[best_idx]:.3f} \\\\\"\n",
    "    )\n",
    "\n",
    "\n",
    "xgb_grid = {\"n_estimators\": [10, 50, 100, 200]}\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expert --- #\n",
    "prob_xgb_fp = run_xgb(fp_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1489d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grover --- #\n",
    "run_xgb(grover_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grover-Large --- #\n",
    "run_xgb(grover_large_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expert-Grover --- #\n",
    "run_xgb(fp_grover_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expert-Grover-Large --- #\n",
    "run_xgb(fp_grover_large_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CFP --- #\n",
    "run_xgb(cfp_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MegaMolBART base --- #\n",
    "run_xgb(mmb_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70983964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MegaMolBART Fine-tuned --- #\n",
    "run_xgb(mmb_ft_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MegaMolBART Fine-tuned CFP --- #\n",
    "run_xgb(cfp_mmb_ft_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN --- #\n",
    "run_xgb(gcn_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN-CFP --- #\n",
    "run_xgb(gcn_cfp_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbcd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN-MMB Fine-tuned --- #\n",
    "run_xgb(gcn_mmb_ft_X, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCN-MMB Fine-tuned-CFP --- #\n",
    "run_xgb(gcn_mmb_ft_cfp, y, split_df, xgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fe9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Val MCC (GCN): {matthews_corrcoef(labels_val, val_preds)}\")\n",
    "print(f\"Test MCC (GCN): {matthews_corrcoef(labels_test, test_preds)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lnp",
   "language": "python",
   "name": "lnp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cb424124a72f34ff51dcdc06273546de873365fee37dc5a1ddc2de1ace99d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
